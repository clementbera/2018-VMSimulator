%%%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt, sigplan]{acmart}
%%\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan,10pt]{acmart}\settopmatter{}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
%\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
%\acmYear{2017}
%\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
%\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
%\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear

%% Bibliography style

%\bibliographystyle{ACM-Reference-Format}

\citestyle{acmnumeric}   


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{multirow}
\usepgfplotslibrary{statistics}
\usepackage{dblfloatfix} %enable fig at bottom of page
%\input{macros.tex}

\usepackage{xcolor}
\newcommand{\todo}[1]{\color{orange}\fbox{\bfseries\sffamily\scriptsize TODO:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\sd}[1]{\color{red}\fbox{\bfseries\sffamily\scriptsize Stef:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\sk}[1]{\color{blue}\fbox{\bfseries\sffamily\scriptsize Sophie:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\cba}[1]{\color{purple}\fbox{\bfseries\sffamily\scriptsize Clement:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
%\newcommand*{rotatebox{75}}
\input{macros}

\begin{document}

%%%
% Keyword definition (so I can change it once for all)
%%%

\def\openSmalltalkVM{Open-Smalltalk-VM\xspace}
%%%
% End Legend
%%%

%% Title information
\title[Over Twenty Years of Virtual Machine Development Through Simulation]{Over Twenty Years of Virtual Machine Development and Debugging Through Simulation}

%% Author with single affiliation.
\author{Eliot Miranda}
                                        %% can be repeated if necessary
\affiliation{
  %\position{Position1}
 % \department{VM team}              %% \department is recommended
  \institution{Feenk}            %% \institution is required
 % \streetaddress{Street1 Address1}
  \city{San Francisco}
  %\state{France}
  %\postcode{Post-Code1}
  \country{California}                    %% \country is recommended
}
\email{eliot.miranda@gmail.com}          %% \email is recommended

%% Author with two affiliations and emails.
\author{Cl\'ement B\'era}
\affiliation{
  % \position{}
	\department{Software Languages Lab}              %% \department is recommended
	\institution{Vrije Universiteit Brussel}            %% \institution is required
	\city{Brussel}
  % \state{}
  % \postcode{}
	\country{Belgium}                    %% \country is recommended
}
\email{clement.bera@vub.ac.be}          %% \email is recommended

%% Author with two affiliations and emails.
\author{Elisa Gonzalez Boix}
\affiliation{
  % \position{}
	\department{Software Languages Lab}              %% \department is recommended
	\institution{Vrije Universiteit Brussel}            %% \institution is required
	\city{Brussel}
  % \state{}
  % \postcode{}
	\country{Belgium}                    %% \country is recommended
}
\email{egonzale@vub.ac.be}          %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}

\openSmalltalkVM was originally written in Smalltalk to specify and example the virtual machine (VM) implementation. The same code base was changed a little bit to allow Smalltalk-to-C compilation, generating effectively the production VM. Two execution models are effectively available, the simulation mode, executing the Smalltalk code on top of a Smalltalk VM, and the production mode, compiling the code to executable through C. Simulation is used to develop and debug the VM. Production is used to release the VM. 

As the VM evolved, by introducing better garbage collector algorithms or a just-in-time compiler, both execution models co-evolved. For example, the simulation mode was extended with a processor simulator to simulate the code generated by the just-in-time compiler. 

In this paper, we detail the VM simulation infrastructure and we report our experience developing and debugging the VM within it. We discuss some of the limitations and how we dealt with it. Then, we focus on two specific use-cases, a bug in the just-in-time compiler and the development of a new full garbage collector compaction algorithm, and we show how the simulation infrastructure helped us. Lastly, we discuss how we use the simulation mode to perform analysis on the runtime, directing some design decisions we make to tune the VM performance.

\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011008</concept_id>
%<concept_desc>Software and its engineering~General programming languages</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10003456.10003457.10003521.10003525</concept_id>
%<concept_desc>Social and professional topics~History of programming languages</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Just-in-Time compiler, Virtual machine, Managed runtime, Tools}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}
\label{sec:intro}

To specify the language and explain how to write the virtual machine (VM) for it, the Smalltalk-80 crew wrote a Smalltalk VM entirely in Smalltalk \cite{blueBook}. In 1995, the same team built an open-source Smalltalk and VM, Squeak \cite{SqueakByExample}. The VM was ported from the original specifications however part of the code base was narrowed down to a subset of Smalltalk, called \emph{Slang}, to allow Smalltalk to C compilation \cite{BackToTheFuture}. Effectively, the VM could be both simulated by executing the Slang code on top of the Smalltalk VM or compiled to native code to produce the production VM. At this point, the VM consisted mainly in an interpreter, a memory manager with a generational garbage collector (GC) and BitBlt, an extension for the user interface bit-based engine. These tree parts of the VM were written entirely in Slang. A few extra features, such as file management, were written both in Smalltalk for simulation purposes and in C for the production VM.

Over the years, the Squeak VM evolved to give birth recently to \openSmalltalkVM\footnote{https://github.com/OpenSmalltalk/opensmalltalk-vm/}, the default VM for different Smalltalk and Smalltalk-like runtimes (Pharo \cite{PharoByExample}, Squeak \cite{SqueakByExample}, Cuis, Croquet and NewSpeak \cite{NewspeakOopsla}). As the VM evolved, the simulator co-evolved as a tool to develop and debug the VM. The most significant evolution of the simulator came with the introduction of the Just-In-Time compiler (JIT), \emph{Cogit}, a template-based JIT. The existing simulator was not able to interpreter the machine code generated by the JIT and had to be extended by binding various processor simulators (Bochs for x86 \& x64, SkyEye for ARMv6).

In the following section, we explain the VM infrastructure with both the compilation pipeline to generate the production VM and the simulation infrastructure used to develop and debug the VM. Section \ref{sec:GCExp} reports our experience developing the full GC with our infrastructure. Section \ref{sec:JITExp} explains how we fixed a bug in the Just-in-Time compiler using single stepping in machine code in the simulator. Section \ref{sec:Analysis} shows how we abuse the simulator to analyse the runtime and direct our performance decisions. Lastly, we discuss some related work and conclude.

\section{Virtual Machine Infrastructure}
\label{sec:VMSimulation}

As shown in Figure \ref{fig:VMCompilation}, the VM code base is written both in Slang and in C. The Core VM code, also known as the object engine, is written in Slang and includes mainly the interpreter, the template-based JIT and the memory manager. The platform code, \emph{i.e.,} Operating System dependent code such as file management or I/O is written directly in C. 
%EXPLAIN scientifically in this part - no history. + FIGURE, basic with design. 

\begin{figure}[bth!]
		\centering
		\includegraphics[width=0.62\linewidth]{figures/VMCompilation}
		\caption{Cog VM compilation}
		\label{fig:VMCompilation}
\end{figure}

The VM executable is generated in a two step process. Firstly, the Slang-to-C compiler translates the Slang code to C code in a few files. This first compilation pass takes a few seconds. Secondly, the C compiler (depending on the platform, LLVM or GCC), translates the C code into an executable. This second compilation pass may take several dozens of seconds the first time, then, it depends on what the programmer changed and what the C compiler can cache or not, but it usually also takes a few seconds to recompile the C code. The VM can be compiled in two main flavors, interpreter-only or interpreter+JIT. Although the version with the JIT is used in production, the interpreter version is convenient for special purposes, for example to debug the garbage collector or to evaluate new language features without dealing with the JIT complexity.

There are multiple reasons why the object engine is written in Slang and not in C. One of the reason is that the Slang-to-C compiler, using différents annotations, generates C code slightly different from the Slang code: for example, it duplicates the implementation of specific methods with specific constant operands in Slang to generate more efficient code in the interpreter. The main reason is VM simulation. By interpreting the Slang code as Smalltalk code, emulating native code using an external processor simulator and simulating the memory using a large byte array, we are able to simulate the whole VM execution.

\subsection{Simulating the Virtual Machine}

in this section, we first describe briefly the start-up sequence of \openSmalltalkVM using snapshots. Then, we detail the simulation of the interpreter-only VM. The simulation of the full VM is a superset and is explained in the last subsection.

\paragraph{Start-up from snapshots.} 
Smalltalk is a snapshot-based language. A Smalltalk program is started from a snapshot, \emph{i.e.,} a memory dump of all live objects at a given point in the execution of a program. The snapshot includes objects such as the classes, the complied methods in the form of bytecodes and the running processes. At start-up, the VM restores the state of all objects in memory and resumes execution in the active process at snapshot time. 

When programming with Smalltalk, the programmers usually start ups from a snapshot which contains the core libraries, the development environment and the application developed. Development of the application consists essentially in writing and editing code, which effectively installs, modifies and removes compiled methods and classes from the snapshots. When the changes are done, the developer takes a new snapshot, which includes its changes. Deployment is done from a snapshot containing the deployed application and required core librairies (unused librairies and development tools may be removed from the deployed snapshot).

Starting up the VM for source files has never been possible. However, recent work~\cite{PharoBootstrap} allowed one of the Smalltalk runtime to recreate a snapshot from sources, indirectly allowing to start the VM from source files.

\paragraph{Interpreter-only simulation.} 
As the VM can be compiled with and without the JIT, the VM can also be simulated with or without the JIT. 

The heap is simulated as a large contiguous byte array. References between objects are effectively indexes inside the byte array instead of pointers. All the C variables are simulated as Smalltalk objects. They used specific classes, such as \emph{CArrayAccessor} over normal Smalltalk classes, to emulate the C behavior (only array accesses are available in C, not high level iterator APIs for example). One of the most complex component is the Stack. The Stack is represented in \openSmalltalkVM as a double linked list of stack pages which are maintained by the VM. Each stack page is represented as a Smalltalk object.

All the Slang code is implemented in multiple Smalltalk classes, to organise the code. For production, at Slang-to-C compilation time, all the code is compiled in a single C file. No polymorphism is available at runtime. However, polymorphism can be abused for debugging purposes. For example, a class is available holding all the garbage collection compaction logic. This class might be subclassed with a simulation only version: this version can use normal Smalltalk code over Slang, allowing to express easily specific constraints which leads to assertion failures if not met, such constraints are convenient for debugging.

In Simulation, one of the core feature is to be able to re-use the whole Smalltalk IDE, including the browser, the inspectors and the debugger to develop and debug the VM. Most new features can be develop and debug interactively, adding code to the VM at runtime, in the simulation environment, as for normal Smalltalk programs.

\paragraph{JIT simulation.} 

In addition to the interpreter simulator, simulating the JIT requires to simulate the execution of native code it generates. The JIT itself is written in Slang and simulated with the Smalltalk execution model. To simulate the machine code, the start of the byte array representing the memory now holds the machine code generated and installed by the JIT. Bindings to processor simulator libraires (Bochs for x86 and x64, Skyeye for ARMv6) were implemented so that the machine code can be simulated. Calls in-between slang code and direct machine code are a little bit trickier to simulate. Calls from machine code to slang code are implemented by using multiple invalid processor instructions, leading to a trap in the processor simulator. This trap is caught is the VM simulator, which then resumes Slang simulation by calling the method corresponding to the invalid processor instruction. Calling machine code from Slang requires to start the simulation and then launch an exception to stop Slang simulation. The processor simulator can be start in two different ways. It can either start simulating code until it meets an invalid instruction or simulate one instruction at a time. The second version is slower, but allows to implement specific debugging features, such as conditional breakpoints in-between each machine instruction.

\subsection{In-image compilation}

Simulating the whole VM requires going through the whole start-up sequence: loading the snapshot, running code registered in the start-up sequence and resuming the user interface. The whole start-up takes around 15 seconds on a recent Macbook pro. While developing the template JIT, this start-up time may still be too long and move the live programming experience to an edit-compile-run cycle. To work around this problem, we implemented a tool called \emph{In-image compilation}. In-image compilation basically allows to call the JIT as a Smalltalk library on a given bytecode compiled method to generate the corresponding machine code and display the disassembly. Since the JIT is template-based, in-image compilation is very convenient to develop and optimize each of the JIT templates. To generate the machine code, the JIT has to access specific objects (the compiled method, the literals, known objects such as true, false or nil) as if they were in the simulated memory. To work around this, we built a facade, which is started on the compiled method and only includes the subset of objects required by the JIT to generate the machine code.

\subsection{Virtual Machine Simulation Limitations}

\paragraph{Performance.}
The first limitation is due to the simulation performance. The interpreter-only is simulator is around 200 times slower to execute code than the normal VM. With the JIT and processor simulation enabled, without specific debugging options such as conditional breakpoints in between machine instructions, simulation drops to around 500 times slower than the normal VM. This means for example that if a GC bug happens in an application 15 minutes after start-up, it will take 50 hours to reproduce in the interpreter-only simulator. Bugs in the jitted code are worst. Fortunately, we work around this problem by using snapshots and the interpreter-only simulator for GC bugs. In general, once we are able to reproduce a bug in the production VM, we try to snapshot the runtime just before it crashes. The VM simulator can then be started just before the crash and the debugging tools can be used after only several dozens of seconds. If the bug is unrelated to the JIT, the interpreter-only simulator can be used and it is a little bit quicker to execute code.

\paragraph{Calls to external code.}
Although most of the GC and JIT development and bug resolution can be done in the simulator, specific developments and bugs cannot. Basically, any calls outside of the machine code generated by the JIT and the Slang code cannot be simulated. For specific small parts of the VM, such as file management, we extended the simulator, effectively duplicating the code base with the C code, to support those features in simulation. However, there is no solution in the general case: we cannot afford to simulate both the compiled C code and the jitted code on the processor simulator, that would be horribly slow, and specific behaviors in the machine code not present in the code generated by the JIT cannot be simulated (Access to C variables, OS, etc.).

The main limitation we have is with Foreign Function Interfaces (FFI). Most bugs we have with FFI are due to specific interaction between call-backs, low-level assembly FFI specific glue code and moving objects. Such bugs cannot be debugged, so far, with our simulation infrastructure and we have to rely on gdb/lldb.

\section{Garbage Collection Development}
\label{sec:GCExp}

REf to papers

Example of the GC impl.
Debugging mainly from ST, standard dev tools.

default behavior, cloneOnSavenge/GC
Lemming debugging

Levels of Assertions: ST only (debugging code in St), Slang/C, Node.

\section{Just-in-Time Compiler Development}
\label{sec:JITExp}

\subsection{Debugging crashes with conditional breakpoints in machine code}
Back-in-time debugging of machine state
Conditional stepping.

Ref Sista

\subsection{Optimizing the templates with in-image compilation}
+ ref low overhead RB AND explain template optimized with in-image compilation.

\section{Virtual Machine Analysis: Directing performance decisions}
\label{sec:Analysis}

A side-effect of VM simulation, and specifically to be able to interrupt the simulation and introspect the simulated memory and simulation specific objects, is to be able to analyse the runtime with scripts written on-the-fly. 

\subsection{Analysis example}

One of the first analysis we run is to stop the simulation when the machine code zone reached 1Mb. We then iterated over the machine code zone and investigated what was in. As show in Table \ref{tab:generalAnalysis}, 1752 were compiled to machine code by the template JIT, 6352 sends\footnote{We use the Smalltalk terminology, send, to discuss virtual calls since we are talking about Smalltalk.} are present but 2409 of them are not linked (basically, they have never been used). 

\begin{table} [th]
\centering
\begin{tabular}{l|r}
	Number of methods  & 1752 \\
  \hline
	Number of sends  &  6352 \\
  \hline
	Average number of sends per method  & 3.63  \\
  \hline
	Number of unlinked sends  &  2409 \\
  \hline
	Percentage of unlinked sends  & 37.9\%  \\
  \end{tabular}
\caption{General Machine Code Zone Analysis\vspace{-0.5cm}}
\label{tab:generalAnalysis}
\end{table}

Further analysis, in Table \ref{tab:polyAnalysis}, confirms Urs H\"{o}lzle analysis \cite{PICSelf}: around 90\% of used send sites are monomorphic, 9\% are polymorphic (up to 6 different cases in our implementation) and the remaining \% is megamorphic.

\begin{table} [th]
\centering
\begin{tabular}{l|c|c}
  	& Number of sends & \% of linked sends \\
  \hline
	Monomorphic & 3566 & 90.4 \% \\
	Polymorphic & 307 & 07.8 \% \\
	Megamorphic & 70 & 01.8 \% \\
  \end{tabular}
\caption{Polymorphism Inline Cache Analysis\vspace{-0.5cm}}
\label{tab:polyAnalysis}
\end{table}

The code for these analysis are detailed in the Section "Let Me Tell You All About It, Let Me Quantify" of the blog post "Build me a JIT as fast as you can"\footnote{http://www.mirandabanda.org/cogblog/2011/03/01/build-me-a-jit-as-fast-as-you-can/}.

\subsection{Directing the VM behavior}

The results of the analysis are used to direct performance design decisions on the VM. In this section we describe how the analysis impacted a design called "Early Polymorphic inline cache promotion".

We designed the polymorphic inline caches (PICs) with two implementations:
\begin{itemize}
	\item \emph{Closed PICs:} Such caches can deal with up to 6 cases, and are basically implemented as a jump table.
	\item \emph{Open PICs:} Such caches can deal with any number of cases, they consist of three probes searching the global look-up cache, a hash map shared with the interpreter, and fall back into a standard look-up routine if there is a cache miss.
\end{itemize}	

One idea we had was to promote a monomorphic inline cache straight to an open PIC if available, and create the closed PIC only if no open PIC is available for the given selector. The benefit is avoiding lots of code space modifications and an allocation. The downside is replacing faster closed PIC dispatch with slower open PIC dispatch. The question is how many send sites would be prematurely promoted to megamorphic, or how many closed PICs have selectors for which there are open PICs. Analysing the question is easy in our context.

The analysis result showed that 17\% of polymorphic send sites would get prematurely promoted. So we have implemented a simple sharing scheme. The JIT maintains a linked list of open PICs, and before it creates a closed PIC for a send site it will patch it to an open PIC if the list contains one for the send?s selector.

\section{Related Work and Conclusion}

Many VM developers developed different tools to help them being more efficient, but they rarely publish about it. We focus in this section on two related work.

\paragraph{Maxine Inspectors.} The Maxine inspectors \cite{MaxineInspector} were demonstrated at OOPSLA'18. They allow to inspect the running state of the VM while it runs for debugging purposes. One of the main difference with out design is that the Maxine VM is metacircular, they do not have a simulation and a production mode as we do. We believe having two different modes allows us to easily generate a production artifact while still having nice debugging features. Having a full metacircular VM would be interesting. However, so far, most VMs used in production (Java, Javascript, etc.), even after the huge recent investments in the Javascript VMs by the four major web vendors, are still compiling through the C/C++ compiler and not metacircular. Hence, although a metacircular VM has interesting advantages, it is not clear it is that convenient to build a VM in such a way.

\paragraph{RPython toolchain.}  The RPython toolchain \cite{RPythonToolchain} was designed and implemented quite similarly to \openSmalltalkVM. Most of the VM code is written in RPython, a restricted Python, instead of Slang, and some leftovers are written in plain C. The main difference is that RPython is much closer to Python than Slang is to Smalltalk, RPython allows higher level structures such as dictionaries to be used. The design decision comes with its set of advantages and drawbacks. The key advantage is that the RPython code feels like Python code and is relatively quite easy to read write. The main drawback is that RPython to C compilation takes way longer than the Slang to C compilation (up to 40 minutes in a recent Macbook pro for the RSqueak VM \cite{RSqueak}, instead of several seconds for Slang). 

Although the RPython code can be executed as normal Python code, for some reasons, the developers seem to think it is not worth to do it, mostly because executing code in such a way is very slow. The overall architecture of the RPython toolchain is different, which leads to a longer time to reach peak performance (though their peak performance is at least in theory better than with a template JIT as \openSmalltalkVM features). This time may be very significant in simulation mode. In addition, RPython was originally designed for Python, which does not feature snapshot by default, so they cannot abuse snapshots to work around the simulation slow performance.

\subsection*{Conclusion}

We introduced and discussed the \openSmalltalkVM simulation infrastructure, used to develop and debug the VM. We believe it is a powerful tool allowing to reduce our development time and to allow to fix bugs quickly. In the near future, we plan to extend the simulator with customizable development tools, especially the moldable inspectors and debuggers \cite{MoldableInspector, MoldableDebugger}, to have a fancy user interface on top of the current simulation model, currently quite tricky to apprehend by new developers.

%% Acknowledgments
%%\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
%  This material is based upon work supported by the
%  \grantsponsor{GS100000001}{National Science
%    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%  conclusions or recommendations expressed in this material are those
%  of the author and do not necessarily reflect the views of the
%  National Science Foundation.
%\end{acks}

%% Bibliography
\bibliographystyle{alpha}
\bibliography{sista}

\end{document}
